Options
=======

Strategy
-----------------

We provide implementations of Stochastic RBF (SRBF), DYCORS,
Expected Improvement (EI), lower confidence bound (LCB) and random search (RS).
EI can only be used in combination with GPRegressor since uncertainty predictions
are necessary. All strategies support running in serial, batch synchronous parallel,
and asynchronous parallel.

New optimization strategies can be implemented by inheriting from SurrogateBaseStrategy
and implementing the abstract generate_evals method that proposes num_pts
new sample points:

- Required methods
    * generate_evals(num_pts): Proposes num_pts new samples.

The following strategies are currently supported:

SRBFStrategy
^^^^^^^^^^^^
This is an implementation of the SRBF strategy by Regis and Shoemaker:

| Rommel G Regis and Christine A Shoemaker.
| A stochastic radial basis function method for the global optimization of expensive functions.
| INFORMS Journal on Computing, 19(4): 497-509, 2007.

| Rommel G Regis and Christine A Shoemaker.
| Parallel stochastic global optimization using radial basis functions.
| INFORMS Journal on Computing, 21(3):411-426, 2009.

The main idea is to pick the new evaluations from a set of candidate
points where each candidate point is generated as an N(0, sigma^2)
distributed perturbation from the current best solution. The value of
sigma is modified based on progress and follows the same logic as in many
trust region methods; we increase sigma if we make a lot of progress
(the surrogate is accurate) and decrease sigma when we aren't able to
make progress (the surrogate model is inaccurate). More details about how
sigma is updated is given in the original papers.

After generating the candidate points we predict their objective function
value and compute the minimum distance to previously evaluated point. Let
the candidate points be denoted by C and let the function value predictions
be s(x_i) and the distance values be d(x_i), both rescaled through a linear
transformation to the interval [0,1]. This is done to put the values on the
same scale. The next point selected for evaluation is the candidate point
x that minimizes the weighted-distance merit function:

.. math::

    \text{merit}(x) := w s(x) + (1 - w) (1 - d(x))

where :math:`0 \leq w \leq 1`. That is, we want a small function value prediction and a
large minimum distance from previously evalauted points. The weight w is
commonly cycled between a few values to achieve both exploitation and
exploration. When w is close to zero we do pure exploration while w close
to 1 corresponds to explotation.

- Parameters:
    * max_evals: Evaluation budget (int)
    * opt_prob: Optimization problem object, must implement OptimizationProblem
    * exp_design: Experimental design object, must implement ExperimentalDesign
    * surrogate: Surrogate object, must implement Surrogate
    * asynchronous: Whether or not to use asynchrony (True / False).
    * batch_size: Size of the batch. This value is ignored if asynchronous is True. Use 1 for serial or run with asynchronous set to True.
    * extra_points: n Extra points to add to the experimental design (numpy.array of size n x dim)
    * extra_vals: Values for extra_points. Set elements to np.nan if unknown (numpy.array of size n x 1)
    * reset_surrogate: Specify whether or not we are resetting the surrogate model i.e., removing current points (True / False)
    * weights: Weights for merit function (list or numpy.array). Default is [0.3, 0.5, 0.8, 0.95]
    * num_cand: Number of candidate points (int). Default = 100*dim

DYCORStrategy
^^^^^^^^^^^^^

This is an implementation of the DYCORS strategy by Regis and Shoemaker:

| Rommel G Regis and Christine A Shoemaker.
| Combining radial basis function surrogates and dynamic coordinate search in
  high-dimensional expensive black-box optimization.
| Engineering Optimization, 45(5): 529-555, 2013.

This is an extension of the SRBF strategy that changes how the candidate
points are generated. The main idea is that many objective functions depend
only on a few directions so it may be advantageous to perturb only a few
directions. In particular, we use a perturbation probability to perturb a
given coordinate and decrease this probability after each function
evaluation so fewer coordinates are perturbed later in the optimization.

The parameters are the same as in the SRBF strategy.

SOPStrategy
^^^^^^^^^^^

This is an implementation of the SOP strategy by Krityakierne, Akhtar and Shoemaker:

| Tipaluck Krityakierne, Taimoor Akhtar and Christine A. Shoemaker.
| SOP: parallel surrogate global optimization with Pareto center selection
  for computationally expensive single objective problems.
| Journal of Global Optimization, 66(3): 417-437, 2016.

The core idea of SOP is to maintain a ranked archive of all previously evaluated points,
as per non-dominated sorting between two objectives, i.e., i) Objective function value(minimize)
and ii) Minimum distance from other evaluated points(maximize). A sub-archive of center points
is subsequently maintained via selection from the ranked evaluated points. The number of points
in the sub-archive of centers should be equal to (or greater than) the number of parallel threads.
Candidate points are generated around each
